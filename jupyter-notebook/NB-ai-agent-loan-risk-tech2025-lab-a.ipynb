{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Tech 2025 Lab\n### Lab A - Loan Risk AI Agent\nAn **AI Agent** refers to a system or program that is capable completing a request by first planning the sequence of workflow steps and then performing the tasks in the steps by utilizing available tools. AI agents utilizes large language models (LLM) to understand the context of the request, plan and perform the tasks. \n\nThis Python Notebook provides the code to create a simple AI agent for risk and interest rate evaluation in a bank loan processing scenario. It uses LangGraph graph for state and runtime processing. Once created, you assume the persona of a loan risk analyst at a bank and ask the Loan Risk AI Agent questions in natural language to assess risks for customers. When a question is asked, the AI agent will use a LLM to understand the context, determine the seqence of steps, and then complete the steps utilizing availble tools. The tools fetch customer information, credit score, account status and determine risk and interest rate. The sequence of steps, tool calling and final response can be reviewed in the output.\n\n#### To use this Notebook:\n\nRun each cell below one by one and make sure it completes successfully.\n\nWhen prompted for API key enter it in the box and hit Enter and then continue running the cells to initialize the code.\n\nFinally, when prompted enter your query and run the subsequent cells. Some example questions are provided.\n\nYou may run/repeat the query cells by entering and trying different queries.", "metadata": {"id": "4be62f6a-4776-4dc9-b6db-9673f06bc388"}}, {"cell_type": "markdown", "source": "#### Lab A - Exercise 1\nBelow is an outline of the code. The code is set up in Python Notebook cells.\n\n1. Set up required libraries\n2. Define functions to get and check credentials\n3. Define tools that the AI agent can use\n4. Configure the LLM\n5. Define the LangGraph graph and functions for state and runtime processing.\n6. Show a visual representation the graph - the AI agent with tools\n7. Use the AI agent - ask the AI agent risk related question\n8. Review the responses from AI agent risk\n\n#### Lab A - Exercise 2\n1. Add another tool that can provide interest rate.\n2. Use the AI agent - ask the AI agent risk and interest rate related question\n3. Review the responses from AI agent risk\n\n", "metadata": {"id": "609b3c27-f6dc-4daf-8101-7a3b38791717"}}, {"cell_type": "markdown", "source": "### Lab A - Exercise 1", "metadata": {"id": "956b5a54-4eb6-424f-83cc-1f6c15af4aa4"}}, {"cell_type": "markdown", "source": "#### 1. Set up required libraries", "metadata": {"id": "1f742f2c-386c-4094-874a-9be788685811"}}, {"cell_type": "code", "source": "# Set up libraries\n%pip install langgraph==0.2.73\n%pip install -U langchain-ibm==0.3.6\n    \nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import HumanMessage\n\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\nimport requests\nimport json\nimport time\nimport random", "metadata": {"id": "ac1bb4df-737d-45fb-9e14-6bdc75bad441", "scrolled": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### 2. Define functions to get and check credentials", "metadata": {"id": "c376e0e3-76f0-4079-b382-995410034157"}}, {"cell_type": "code", "source": "# Function to get credentials\nimport getpass\nimport os\n\ndef _set_if_undefined(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n\n# Get credentials\n# Set the Project IDs\n# If using you own account, or lab account and project, the current project id will be configured and set automtically\n_set_if_undefined(\"PROJECT_ID\")\n\n# When prompted for API key enter it in the box and hit Enter, move to next cell and then continue running the cells\n_set_if_undefined(\"WATSONX_API_KEY\")\n", "metadata": {"id": "52a152ce-7efd-45f2-b5df-4ce5ea5efab7"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Print credential configurations for validation\nprint('Api key:',os.environ.get(\"WATSONX_API_KEY\"))\nprint('Project id:',os.environ.get(\"PROJECT_ID\"))", "metadata": {"id": "069d3ba7-2788-4f3d-9083-df43c0495ff8"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### 3. Define tools that the AI agent can use", "metadata": {"id": "eb45e773-d3c9-42ca-b349-d062d808e596"}}, {"cell_type": "code", "source": "# Define tools that the AI agent can use\n\n# The tool get_customer_info() simulates/mocks API call that fetches credit score and account status information about a customer\n# The tools get_credit_score() and get_account_status() use the customer info tool to get the score and account status\n# The customer's loan risk is determined based on credit score and account status risk rules using get_overall_risk() tool\n# When a natural language question is asked, the AI agent first determines the context and then the sequence to utilize one or more tools to provide a response \n\n# this tool simulates an API call that will fetch information about customers\n@tool\ndef get_customer_info(customer_id: str): \n    \"\"\"Retrieve customer information based on the customer_id.\"\"\"\n    customer_id = customer_id.lower()\n    \n    if customer_id in ['loren@ibm.com', 'loren', '1111']:\n        return {'credit_score': 455, 'account_status': 'good-standing'}\n    elif customer_id in ['matt@ibm.com', 'matt', '2222']:\n        return {'credit_score': 685, 'account_status': 'closed'}\n    elif customer_id in ['hilda@ibm.com', 'hilda', '3333']:\n        return {'credit_score': 825, 'account_status': 'delinquent'}\n    else:\n        #return {'credit_score': None, 'account_status': None}\n        #if any other customer id is used, provide a random credit score and account status\n        return {'credit_score': random.randint(300, 850), 'account_status': random.choice(['delinquent', 'good-standing', 'closed' ])}\n\n\n\n# this tool fetches credit score of a customer\n@tool\ndef get_credit_score(customer_id: str) -> int: \n    \"\"\"Get the credit score for the customer using customer_id. Customer's name can be used instead of customer_id.\"\"\"\n    customer_info = get_customer_info(customer_id)\n    return customer_info['credit_score']\n    customer_id = customer_id.lower()\n\n\n# this tool fetches account status of a customer\n@tool\ndef get_account_status(customer_id: str) -> str: \n    \"\"\"Get the account status for the customer using customer_id. Customer's name can be used instead of customer_id.\"\"\"\n    customer_info = get_customer_info(customer_id)\n    return customer_info['account_status']\n    customer_id = customer_id.lower()\n\n# this tool determines the overall risk for a customer\n@tool\ndef get_overall_risk(credit_score: int, account_status: str):\n    \"\"\"Get overall risk based on combination of both credit score and account status. Only use high, medium or low as risk categories. Explain how the overall risk was calculated. If the credit score and account status are unknown then do not provide the risk status and first retrieve the missing credit score or account status.\"\"\"\n\n    #print(\"get_overall_risk():credit score:account status::\", credit_score, account_status)\n    \n    if (credit_score > 299 & credit_score < 674): \n        if (account_status == 'delinquent'): overall_risk = 'high'\n        elif (account_status == 'closed'):  overall_risk = 'high'\n        elif (account_status == 'good-standing'):  overall_risk = 'medium'\n        else: overall_risk = 'unkown'\n    \n    elif (credit_score >= 675 & credit_score < 749): \n        if (account_status == 'delinquent'): overall_risk = 'high'\n        elif (account_status == 'closed'):  overall_risk = 'medium'\n        elif (account_status == 'good-standing'):  overall_risk = 'medium'\n        else: overall_risk = 'unkown'\n\n    elif (credit_score >= 750 & credit_score < 851): \n        if (account_status == 'delinquent'): overall_risk = 'high'\n        elif (account_status == 'closed'):  overall_risk = 'low'\n        elif (account_status == 'good-standing'):  overall_risk = 'low'\n        else: overall_risk = 'unkown'\n    \n\n    else: overall_risk = 'unable to determine.'    \n    return overall_risk\n\n# Set the list of tools to be used by the AI agent\ntools = [get_credit_score, get_account_status, get_overall_risk, get_customer_info]\n\ntool_node = ToolNode(tools)\n\n", "metadata": {"id": "4aaf0378-a97b-4774-bc8d-855f75c9e3f9"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### 4. Configure the LLM", "metadata": {"id": "479bcf27-bfab-4602-9cbc-842311bb64c6"}}, {"cell_type": "code", "source": "#Configure the LLM\n\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames\nparameters = {\n    GenTextParamsMetaNames.DECODING_METHOD: \"sample\", #\"greedy\", #\"sample\"\n    GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,\n    GenTextParamsMetaNames.MAX_NEW_TOKENS: 250,\n    GenTextParamsMetaNames.TEMPERATURE: 0,\n    #GenTextParamsMetaNames.TOP_K: 50,\n    #GenTextParamsMetaNames.TOP_P: 1,\n}\n\nfrom langchain_ibm import ChatWatsonx\nmodel = ChatWatsonx(\n    model_id=\"mistralai/mistral-large\", \n    #url=\"https://us-south.ml.cloud.ibm.com\", \n    url=\"https://eu-de.ml.cloud.ibm.com\", \n    apikey=os.environ.get(\"WATSONX_API_KEY\"),\n    project_id=os.environ.get(\"PROJECT_ID\"),\n    params=parameters,\n).bind_tools(tools)", "metadata": {"id": "bbe66af5-01dd-44c2-9d61-c7cbc7758cf5"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### 5. Define the LangGraph graph and functions for state and runtime processing.", "metadata": {"id": "0a89083d-9436-45ff-851d-6c5698c8b0f7"}}, {"cell_type": "code", "source": "# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state['messages']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state['messages']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n", "metadata": {"id": "b2959dec-c28c-4ad5-82a0-d5ab8e33c8f8"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# add a normal edge from `tools` to `agent`.\n# after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"tools\", 'agent')\n\n\n# Compile graph\napp = workflow.compile()\n", "metadata": {"id": "fd9f88ac-fc71-4939-8f09-8e3e719450e4"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### 6. Show a visual representation the graph - the AI agent with tools", "metadata": {"id": "d05e046a-8d73-4a29-aa78-6b1c89b563c5"}}, {"cell_type": "code", "source": "# Show graph\nfrom IPython.display import display, Image\ndisplay(Image(app.get_graph().draw_mermaid_png()))", "metadata": {"id": "225b9451-fed0-4e61-9157-66ac42a0468d"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### 7. Use the AI agent - ask the AI agent risk related question\n", "metadata": {"id": "6f8e1264-ad9d-43b1-a0f6-fc41292de435"}}, {"cell_type": "markdown", "source": "\nAsk the AI agent these questions. Review the response.\n\n- what can you do for me?\n- what can you do for me? how?\n\n- what is the risk for matt?\n \n- what is the risk for matt? explain why?\n\n- tell me about hilda?\n  \n- what is the risk for hilda? explain why?\n\n- what is the risk for credit score 774 and account status delinquent?\n\n- what is the risk for credit score 774 and account status delinquent? how was it determined?\n\nAlso try asking questions about interest rate....\n\n- what is the interest rate for matt?\n\n- what is the interest rate for matt? explain how it was determined?\n\nFor the last two questions, note that there is no tool yet that provides interest rate. So the response does not answer the question appropriately. \nWe will add interest rate tool in the next exercise.\n\n#### Write the query in the box and hit Enter and then continue running the next cells", "metadata": {"id": "eb5074ac-de5c-4cc9-ac2c-76b9fd92a900"}}, {"cell_type": "code", "source": "human_query = input(\"Type a question and hit enter: \")", "metadata": {"id": "ca269a6f-1ab0-4950-b27b-5101e52f052b"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Use the runtime\nfinal_state = app.invoke(\n    #{\"messages\": [HumanMessage(content=\"what is the overall risk?\")]},\n    {\"messages\": [HumanMessage(content=human_query)]},\n    config={\"configurable\": {\"thread_id\": random.randint(1, 100)}}\n)\n\n#final_state[\"messages\"][-1].content\n\nfor m in final_state[\"messages\"]:\n    m.pretty_print()\n\n#Print final message\n#final_state[\"messages\"][-1].content\n", "metadata": {"id": "053a334b-5f12-4a92-ab05-7e22e84219c5"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### 8. Review the response above from AI agent risk\nRun/repeat the above cells with different queries, i.e., rerun the human_query and final_state = app.invoke() cells with different queries", "metadata": {"id": "a14b6c66-53f5-4730-8428-24b398177176"}}, {"cell_type": "code", "source": "", "metadata": {"id": "6984b533-9b55-4e89-9bd2-75517e26e4cc"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "3900ccac-113d-4877-82c3-68690f7f3bc7"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "7207c88a-163e-4759-9c1f-c23a87ef6f32"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "<a id='lab_a_exercise_2'></a>\n### Lab A - Exercise 2\n", "metadata": {"id": "cdc2ebe1-36d1-4b66-83cd-6d8801a8dfb7"}}, {"cell_type": "markdown", "source": "So far we had tool that provided the risk level for a customer - as low, medium or high. There was no tool to provide a specific interest rate.\nIn this exercise we will add another tool that can provide the interest rate based on the risk level.\n", "metadata": {"id": "d00f3964-30f8-4451-89b6-46fb5a9ba8fe"}}, {"cell_type": "code", "source": "# this tool determines the interest rate for a customer\n@tool\ndef get_interest_rate(overall_risk: str):\n    \"\"\"Get interest rate percentage based on the overall risk. If the overall risk is not known then do not provide the interest rate and first retrieve the missing overall risk.\"\"\"\n    if (overall_risk.lower() == \"high\"):\n        interest_rate=10.75;\n    \n    elif (overall_risk.lower() == \"medium\"):\n        interest_rate=5.25;\n\n    elif (overall_risk.lower() == \"low\"):\n        interest_rate=3.0;\n    \n    else: interest_rate = 'unable to determine.'    \n    return interest_rate\n\n\n# Lets add/include this tool in the list of tools used by the AI agent in the following code line \"tools =...\"\ntools = [get_credit_score, get_account_status, get_overall_risk, get_interest_rate, get_customer_info]\ntool_node = ToolNode(tools)", "metadata": {"id": "907303b3-dbf0-42ec-9ba1-65c4d57a7004"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Re-bind the model configuration with new tools\n\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames\nparameters = {\n    GenTextParamsMetaNames.DECODING_METHOD: \"sample\", #\"greedy\", #\"sample\"\n    GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,\n    GenTextParamsMetaNames.MAX_NEW_TOKENS: 250,\n    GenTextParamsMetaNames.TEMPERATURE: 0,\n    #GenTextParamsMetaNames.TOP_K: 50,\n    #GenTextParamsMetaNames.TOP_P: 1,\n}\n\nfrom langchain_ibm import ChatWatsonx\nmodel = ChatWatsonx(\n    model_id=\"mistralai/mistral-large\", \n    #url=\"https://us-south.ml.cloud.ibm.com\", \n    url=\"https://eu-de.ml.cloud.ibm.com\", \n    apikey=os.environ.get(\"WATSONX_API_KEY\"),\n    project_id=os.environ.get(\"PROJECT_ID\"),\n    params=parameters,\n).bind_tools(tools)", "metadata": {"id": "83b40917-602b-4725-8b64-799788371f55"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Re-define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state['messages']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n\n# Re-define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state['messages']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}", "metadata": {"id": "fa1564b2-b055-4c9c-8ed1-f7cdb037a74b"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Re-define a new graph with new tools\nworkflow = StateGraph(MessagesState)\n\n# Re-define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# add a normal edge from `tools` to `agent`.\n# after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"tools\", 'agent')\n\n\n# Re-compile graph\napp = workflow.compile()\n", "metadata": {"id": "9803104c-883a-419a-b727-d5b809c22ce8"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "\nAsk the AI agent questions about interest rate. Review the response.\n\n \n- what is the interest rate for matt?\n\n- what is the interest rate for matt? explain how it was determined?\n\nNote that how the AI agent can now use the get_interest_rate to provide interest rate also.\n\n#### Write the query in the box and hit Enter and then continue running the next cells", "metadata": {"id": "7bbd99ea-9163-4e69-a238-cd9bd70b1dd9"}}, {"cell_type": "code", "source": "human_query = input(\"Type a question and hit enter: \")\n", "metadata": {"id": "5799519a-abd5-47cd-b394-35e0554cc744"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Use the runtime\nfinal_state = app.invoke(\n    #{\"messages\": [HumanMessage(content=\"what is the overall risk?\")]},\n    {\"messages\": [HumanMessage(content=human_query)]},\n    config={\"configurable\": {\"thread_id\": random.randint(1, 100)}}\n)\n\n#final_state[\"messages\"][-1].content\n\nfor m in final_state[\"messages\"]:\n    m.pretty_print()\n\n#Print final message\n#final_state[\"messages\"][-1].content", "metadata": {"id": "f9551a68-cd7e-44ab-b820-64e266ad3611"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "d606aec0-2b2f-4cef-b56a-73dddf56a464"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "10e0c369-ead5-45ac-9b3c-c0ee10f3243a"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "802e3518-0156-4350-b553-f819df068056"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "9cec3987-9978-4750-8dbd-7c8ea258feb9"}, "outputs": [], "execution_count": null}]}