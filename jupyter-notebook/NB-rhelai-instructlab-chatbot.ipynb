{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "IBM Tech 2025 Lab\n=================\n\nLAB 2: RHELAI & InstructLab\n---------------------------\n\nThis python notebook is a simple chatbot that uses the requests package to `chat` with a model served on a RHELAI VSI.\n\nIf you are unfamiliar with Jupyter Notebook interface, you can quickly go through [documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/notebook-editor.html?context=wx&locale=en&audience=wdp)\n\nGoals\n-----\n1) Chat with a model served on a RHELAI VSI using InstructLab VLLM\n2) Try a document summarization usecase using external assets\n3) Chat to expose some limitations of base mistral7b model\n4) Chat with a fine-tuned model that has been trained on synthetic generated data (SGD\n\n**Exercise 1.a**: Chat with a Mistral-7b model served on a RHELAI VSI instance.\n\nObserve the Model Server URL (and port) endpoint\nSet the API Key in appropriate cell.  Use the same API key which you used for the previous Lab.\nProvide a prompt to chat with the served model. Type 'exit' or 'quit' to terminate the chat loop", "metadata": {"id": "110b3afc-90c5-4096-8973-b67fc3a4b6ec"}}, {"cell_type": "markdown", "source": "1. Install Pre-requisite libraries", "metadata": {"id": "a315ca74-508e-470b-a705-c9f9b6672d38"}}, {"cell_type": "code", "source": "!pip install requests", "metadata": {"id": "fdc519dc-bfe2-4fc1-a396-030e8e04599b", "scrolled": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import requests\nimport json\nimport getpass", "metadata": {"id": "f00890a3-1eee-4692-831d-7f8095c59e1a"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "2. Set the variables for accessing the model server", "metadata": {"id": "2e156ca4-f397-4968-b38f-0c8e75fab1fb"}}, {"cell_type": "code", "source": "# Define the model server URL (ensure it's correct for your deployment)\nMODEL_SERVER_URL = \"https://ilab-base.fs-demo.biz/v1/chat/completions\"\n\n# Securely get API key or authentication token if required\nAPI_KEY = None", "metadata": {"id": "ec270148-042e-4673-8cc7-4dceb7bdc129"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "3. Define function to chat with model using request package", "metadata": {"id": "3a52e96d-7077-4664-baa1-dd20f937ba62"}}, {"cell_type": "code", "source": "def chat_with_model(prompt):\n    headers = {\"Content-Type\": \"application/json\"}\n    if API_KEY:\n        headers[\"Authorization\"] = f\"Bearer {API_KEY}\"\n    \n    payload = {\n    \"model\": \"rhelai_service_model\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": prompt}\n    ]\n    }\n    \n    try:\n        response = requests.post(MODEL_SERVER_URL, headers=headers, json=payload, verify=True)\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response received.\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return f\"Error communicating with model: {e}\"\n", "metadata": {"id": "b061e89d-3571-464c-a26f-58205f2eff46"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "4. Provide a prompt to chat with the served model. Wait for response. Type 'exit' or 'quit' to terminate the chat loop.", "metadata": {"id": "9d2d07c2-5ceb-4b69-bc14-e2fdb3870236"}}, {"cell_type": "code", "source": "# Chat loop\nwhile True:\n    user_input = input(\"You: \")\n    print(f\"Please wait....\")\n    if user_input.lower() in [\"exit\", \"quit\"]:\n        print(\"Exiting chat.\")\n        break\n    \n    response = chat_with_model(user_input)\n    print(f\"Model: {response}\")", "metadata": {"id": "b89fbf94-671d-45f2-8178-9f559ef3e568", "msg_id": "94df55aa-beb7-46a2-98c1-9d3d6bbed637"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "**Exercise 1.b**: Summarize documents\n\nLoad your external data as assets and use them in your queries. Here we will load the document into associated COS bucket first and then use its contents to form prompts.\n\nClick on ![Image](https://www.ibm.com/docs/en/SSQNUZ_5.1.x/wsj/console/images/find_data_icon.png) to get started. Once data asset has been imported successfully, click on \n![Image](https://www.ibm.com/docs/en/SSQNUZ_5.1.x/wsj/analyze-data/images/code-snippets-icon.png) to generate the code snippet similar to below.\n\nRefer: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/load-and-access-data.html?context=wx&locale=en&audience=wdp\n\nSample Prompts:\n1. This is a conversation between a doctor and patient, provide a diagnosis and plan the next steps. Explain your response.\n2. This is a conversation between a doctor and patient, is there an emergency involved or does this situation need immediate medical attention?\n3. Based on the above conversation between a doctor and patient, what are the major concerns discussed and course of treatment advised.", "metadata": {"id": "c8f4c091-2012-431f-8621-53323c3e5908"}, "attachments": {}}, {"cell_type": "markdown", "source": "5. Insert code snippet to read data", "metadata": {"id": "e8491f46-35a0-4ced-9dc5-ad0e0bb34bef"}}, {"cell_type": "code", "source": "\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\n\ncos_client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='pQT19GjBZoU33LeoUNiT-dAQf8osQUYpHdzs5lWwNQIR',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/identity/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.direct.us-south.cloud-object-storage.appdomain.cloud')\n\nbucket = 'rhelaiinstructlab-donotdelete-pr-tjhwbcblh1zk4y'\nobject_key = 'RES0217.txt'\n\n# load data of type \"text/plain\" into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\n\nstreaming_body_1 = cos_client.get_object(Bucket=bucket, Key=object_key)['Body']", "metadata": {"id": "00452095-fb79-4da9-9e75-ff2985b6cb5e"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "6. Review the document contents", "metadata": {"id": "8b6a7810-2742-418e-ad3e-8d96971a4523"}}, {"cell_type": "code", "source": "document = streaming_body_1.read()\nprint(f\"Document read is \\n {document.decode('utf-8')}\")", "metadata": {"id": "bc162f3e-8607-4074-a0e3-abb7e9267b69"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "6. Use the document read and prompt with it.\n  \nSample Prompts:\n\n1) This is a conversation between a doctor and patient, provide a diagnosis and plan the next steps. Explain your response.\n2) This is a conversation between a doctor and patient, is there an emergency involved or does this situation need immediate medical attention?\n3) Based on the above conversation between a doctor and patient, what are the major concerns discussed and course of treatment advised.", "metadata": {"id": "11481e96-e1bc-490d-a716-97021eb4077d"}}, {"cell_type": "code", "source": "# Chat loop\nwhile True:\n    user_input = input(\"You: \")\n    print(f\"Please wait....\")\n    if user_input.lower() in [\"exit\", \"quit\"]:\n        print(\"Exiting chat.\")\n        break\n    \n    response = chat_with_model(f\"{document} \\n {user_input}\")\n    print(f\"Model: {response}\")", "metadata": {"id": "bd71f5f7-240c-40ad-b92b-6dee1fdde909", "scrolled": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "**Exercise 2:** Ask about IBM Cloud for Financial Services\n\nThe base model is not trained on IBM Cloud for financial services and starts to hallucinate. Prompt to ask about FSCloud\n\nPrompt 1) What is IBM FSCloud and what industry clients is it focussed at?\nPrompt 2) The security control requirements of IBM Cloud for Financial services are based on what?\nPrompt 3) What is IBM Cloud for Financial Services Validated?\n\nNotice how the responses are vague and in some cases downright incorrect. The base model is hallucinating.", "metadata": {"id": "34ee7a0f-0743-432c-8f95-99e93c67f3ec"}}, {"cell_type": "markdown", "source": "7. Set the endpoint for VSI instance serving base model and chat with the model", "metadata": {"id": "15bbdb35-0370-460b-a5f0-a15958423fee"}}, {"cell_type": "code", "source": "# Define the base model server URL (ensure it's correct for your deployment)\nMODEL_SERVER_URL = \"https://ilab-base.fs-demo.biz/v1/chat/completions\"", "metadata": {"id": "53fda5b8-f167-4aa9-8a0a-cf346f96070e"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Chat loop\nwhile True:\n    user_input = input(\"You: \")\n    print(f\"Please wait....\")\n    if user_input.lower() in [\"exit\", \"quit\"]:\n        print(\"Exiting chat.\")\n        break\n    \n    response = chat_with_model(f\"{user_input}\")\n    print(f\"Model: {response}\")", "metadata": {"id": "0c7615e6-93b9-43ef-88a2-de56913f46e3", "scrolled": true}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "**Exercise 3:** The base model is fine-tuned on a knowledge base of fscloud. The fine-tuned model is served on a different endpoint. \n\nRepeat the prompts to learn about IBM Cloud for Financial Services.", "metadata": {"id": "52f4f2e9-ebc0-4138-a273-2b24bc731c79"}}, {"cell_type": "code", "source": "8. Set the endpoint for VSI instance serving the fine tuned model and chat with the model", "metadata": {"id": "a5870ada-b7b1-4eda-86b3-1f4a6e07c6da"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Define the base model server URL (ensure it's correct for your deployment)\nMODEL_SERVER_URL = \"https://ilab-tuned.fs-demo.biz/v1/chat/completions\"", "metadata": {"id": "f01b0973-2093-49c1-b4e2-c29e0c8298ff"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Chat loop\nwhile True:\n    user_input = input(\"You: \")\n    print(f\"Please wait....\")\n    if user_input.lower() in [\"exit\", \"quit\"]:\n        print(\"Exiting chat.\")\n        break\n    \n    response = chat_with_model(f\"{user_input}\")\n    print(f\"Model: {response}\")", "metadata": {"id": "1bd12077-d3a6-49f5-a8bf-65c97a5210be", "scrolled": true}, "outputs": [], "execution_count": null}]}